import sys
import time
import logging
import itertools
from datetime import datetime, timezone
from typing import Iterator, Optional
from delta.tables import DeltaTable
from pyspark.sql import DataFrame
from pyspark.sql.types import StructType, StructField, StringType, LongType, MapType, BooleanType
import pyspark.sql.functions as F
from databricks.sdk import WorkspaceClient
from databricks.sdk.service.sql import QueryFilter, QueryInfo
from databricks.sdk.service.sql import TimeRange
from databricks.connect.session import DatabricksSession

spark = DatabricksSession.builder.getOrCreate()
logger = logging.getLogger(__name__)

class QueryLogger:
    """Gets DBSQL query history from the Databricks API and merges it into a Delta Lake table.

    Attributes:
        catalog (str): Catalog name
        schema (str): Schema name
        table (str): Table name
        start_time (Optional[datetime]): Limit results to queries that started after this time
        end_time (Optional[datetime]): Limit results to queries that started before this time
        user_ids (Optional[list[int]]): A list of user IDs who ran the queries
        warehouse_ids (Optional[list[str]]): A list of warehouse IDs
        include_metrics (bool): Whether to include metrics about query. Defaults to True.
        pipeline_mode (str): If set to 'triggered', code will load data and exit. Otherwise it will load data every 10 seconds. Defaults to 'triggered'.
        backfill_period (str): Controls how far back to look for the initial data load. Defaults to '7 days'.
        reset (str): If set to 'yes', the target table will be replaced. Defaults to 'no'.
        additional_cols (dict): Dictionary of additional columns. Provide the column name as the key, and a SQL expression for the value.
    """

    def __init__(
        self,
        catalog: str,
        schema: str,
        table: str,
        start_time: Optional[datetime] = None,
        end_time: Optional[datetime] = None,
        user_ids: Optional[list[int]] = None,
        warehouse_ids: Optional[list[str]] = None,
        include_metrics: bool = True,
        pipeline_mode: str = 'triggered',
        backfill_period: str = '7 days',
        reset: str = 'no',
        additional_cols: Optional[dict] = None
    ):
        if catalog not in(None, ''):
            self.catalog = catalog
        else:
            raise Exception("A catalog is required but was not provided")
        if schema not in(None, ''):
            self.schema = schema
        else:
            raise Exception("A schema is required but was not provided")
        if table not in(None, ''):
            self.table = table
        else:
            raise Exception("A table is required but was not provided")
        self.schema = schema
        self.table = table
        self.start_time = start_time
        self.end_time = end_time
        self.user_ids = user_ids
        self.warehouse_ids = warehouse_ids
        self.include_metrics = include_metrics
        self.pipeline_mode = pipeline_mode
        self.backfill_period = backfill_period
        self.reset = reset
        self.additional_cols = additional_cols
        self.w = WorkspaceClient()

    def create_target_table(self) -> None:
        """Creates target Delta Lake table"""

        spark.sql(f'use catalog {self.catalog}')
        spark.sql(f'create schema if not exists {self.schema}')
    
        create_tbl_stmt = (
            "create or replace table" if self.reset == "yes" else "create table if not exists"
        )
    
        spark.sql(
            f"""
                {create_tbl_stmt} {self.catalog}.{self.schema}.{self.table} (
                    query_id STRING,
                    status STRING,
                    query_text STRING,
                    query_start_time TIMESTAMP,
                    execution_end_time TIMESTAMP,
                    query_end_time TIMESTAMP,
                    user_id BIGINT,
                    user_name STRING,
                    spark_ui_url STRING,
                    warehouse_id STRING,
                    error_message STRING,
                    rows_produced BIGINT,
                    metrics MAP <STRING,STRING>,
                    is_final BOOLEAN,
                    channel_used MAP <STRING,STRING>,
                    duration BIGINT,
                    executed_as_user_id BIGINT,
                    executed_as_user_name STRING,
                    plans_state STRING,
                    statement_type STRING
                )
                using delta cluster by (query_start_time)
                tblproperties (
                  'delta.enableDeletionVectors' = 'true'
                )
            """
        )
        
        logger.info(f"Created table {self.catalog}.{self.schema}.{self.table} if it did not already exist.")
    
    def _get_time_filter(self) -> None:
        """Gets time filters for query history API
        
        To enable incremental loads, the starting time is obtained from the query_history table in the following order:
        1. min(query_start_time) of queries in 'QUEUED' or 'RUNNING' status within the past 3 days
        2. max(query_start_time)
        3. current_date() - backfill_period (Initial loads only)

        The end time is the current timestamp.
        """

        start_time_query = f"""
            select
              coalesce(
                (
                  select
                    min(query_start_time) as min_start
                  from
                    {self.catalog}.{self.schema}.{self.table}
                  where
                    status in ('QUEUED', 'RUNNING')
                    and query_start_time >= current_date() - interval 3 days
                ),
                max(query_start_time),
                current_date() - interval {self.backfill_period}
              )
            from
              {self.catalog}.{self.schema}.{self.table}
        """
        
        self.start_time = spark.sql(start_time_query).collect()[0][0]
        self.end_time = datetime.now(tz=timezone.utc)
        
        logger.info(f'API filter start time: {self.start_time.strftime("%Y-%m-%d %H:%M:%S")}, end time: {self.end_time.strftime("%Y-%m-%d %H:%M:%S")}')
    
    def get_query_history(self) -> Iterator[QueryInfo]:
        """Gets DBSQL query history using the Databricks Python SDK.

        https://docs.databricks.com/api/workspace/queryhistory/list
        
        Yields:
            Iterator[QueryInfo]: generator containing results from the DBSQL query history API
        """
        
        if self.start_time is None:
            raise RuntimeError('start_time variable cannot be None. It must be provided when creating a QueryLogger instance or set by _get_time_filter() when incremental_load=True in run().')
        if self.end_time is None:
            raise RuntimeError('end_time variable cannot be None. It must be provided when creating a QueryLogger instance or set by _get_time_filter() when incremental_load=True in run().')
        
        start_time_ms = int(self.start_time.timestamp() * 1000)
        end_time_ms = int(self.end_time.timestamp() * 1000)
        
        logger.info(f'Retrieving query history. This can take a while with larger data volumes.')

        query_hist_list = self.w.query_history.list(
            include_metrics=self.include_metrics,
            max_results=1000,
            filter_by = QueryFilter(
                user_ids=self.user_ids,
                warehouse_ids=self.warehouse_ids,
                query_start_time_range = TimeRange(
                    start_time_ms=start_time_ms, end_time_ms=end_time_ms
                )
            )
        )

        return query_hist_list
    
    def create_dataframe(self, query_hist_list) -> DataFrame:
        """Creates dataframe from query history API response data.

        Data is minimally processed. Unix timestamps are converted to a human readable datetime.
        
        Args:
            query_hist_list (generator): generator from the Python SDK WorkspaceClient().query_history.list()
        
        Returns:
            DataFrame: DataFrame containing DBSQL query history
        """

        query_hist_list = (i.as_dict() for i in query_hist_list)
        
        df_schema = StructType(
            [
                StructField("query_id", StringType(), True),
                StructField("status", StringType(), True),
                StructField("query_text", StringType(), True),
                StructField("query_start_time_ms", LongType(), True),
                StructField("execution_end_time_ms", LongType(), True),
                StructField("query_end_time_ms", LongType(), True),
                StructField("user_id", LongType(), True),
                StructField("user_name", StringType(), True),
                StructField("spark_ui_url", StringType(), True),
                StructField("warehouse_id", StringType(), True),
                StructField("error_message", StringType(), True),
                StructField("rows_produced", LongType(), True),
                StructField("metrics", MapType(StringType(), StringType(), True), True),
                StructField("is_final", BooleanType(), True),
                StructField("channel_used", MapType(StringType(), StringType(), True), True),
                StructField("duration", LongType(), True),
                StructField("executed_as_user_id", LongType(), True),
                StructField("executed_as_user_name", StringType(), True),
                StructField("plans_state", StringType(), True),
                StructField("statement_type", StringType(), True)
            ]
        )
        
        query_hist_df = spark.createDataFrame(query_hist_list, df_schema)

        query_hist_parsed_df = (
            query_hist_df
            .withColumn("query_start_time", F.expr("to_timestamp(query_start_time_ms / 1000)"))
            .withColumn("execution_end_time", F.expr("to_timestamp(execution_end_time_ms / 1000)"))
            .withColumn("query_end_time", F.expr("to_timestamp(query_end_time_ms / 1000)"))
            .drop(*['query_start_time_ms', 'query_end_time_ms', 'execution_end_time_ms'])
        )
        
        if self.additional_cols: 
            for k,v in self.additional_cols.items():
                query_hist_parsed_df = query_hist_parsed_df.withColumn(k, F.expr(f"{v}"))
        
        return query_hist_parsed_df
    
    def load_query_history(self, query_hist_parsed_df: DataFrame) -> None:
        """Merges data into a Delta Lake table. Schema evolution is enabled.
        
        The target table is filtered using query_start_time >= '{query_start_time}' to reduce the search space for matches and enable file skipping.
        This optimization is possible because the source data (With the exception of the initial load) will only contain updates for queries that were previously in progress.
        
        Args:
            query_hist_parsed_df (DataFrame): DataFrame containing DBSQL query history
        """

        spark.conf.set('spark.databricks.delta.schema.autoMerge.enabled', True)
        tgt_table = DeltaTable.forName(spark, f'{self.catalog}.{self.schema}.{self.table}')
        
        if self.start_time is None:
            raise RuntimeError('start_time variable cannot be None. It must be provided when creating a QueryLogger instance or set by _get_time_filter() when incremental_load=True in run().')
        query_start_time = self.start_time.strftime("%Y-%m-%d")

        logger.info(f"Merging data into {self.catalog}.{self.schema}.{self.table}. Target table filtered using query_start_time >= '{query_start_time}' to optimize the merge.")
        
        merge = (
            tgt_table.alias("t")
            .merge(query_hist_parsed_df.alias("s"), f"t.query_id = s.query_id and t.query_start_time >= '{query_start_time}'")
            .whenMatchedUpdateAll()
            .whenNotMatchedInsertAll()
            .execute()
        )

        logger.info(f"Merge completed")

    def optimize(self) -> None:
        """Optimize Delta Lake table
        
        The target table uses Liquid Clustering by default. If ZORDER is used, update the optimize command below to include ZORDER columns.
        """

        spark.sql(f'optimize {self.catalog}.{self.schema}.{self.table}')
        logger.info(f'Optimized table {self.catalog}.{self.schema}.{self.table}')
    
    def run(self, filter_current_user: bool = False, incremental_load: bool = True, trigger_interval: int = 30) -> None:
        """Runs DBSQL query logger pipeline

        Args:
            filter_current_user (bool): If set to true, only queries for the current user will be retrieved. Defaults to false.
            incremental_load (bool): If set to true, data retrieval will be incremental. Defaults to true.
            trigger_interval (int): number of seconds to wait between each run in continuous mode. Defaults to 30 seconds.
        
        Target table will be optimized following merge. If running in continuous mode, optimize will be performed every 8 merges. 
        """
        
        self.create_target_table()

        if filter_current_user:
            current_user = self.w.current_user.me()
            current_user_id = int(current_user.id or 0) # Default to 0 to avoid None
            if current_user_id == 0:
                raise RuntimeError('Failed to get current user')
            logger.info(f'Filtering to current user id {current_user.user_name}')
            self.user_ids = [current_user_id]

        for i in itertools.count(start=1):
            if i % 8 == 0:
                self.optimize()
            
            if incremental_load:
                self._get_time_filter()
            
            query_hist = self.get_query_history()
            query_hist_df = self.create_dataframe(query_hist)
            self.load_query_history(query_hist_df)
            
            if self.pipeline_mode == 'triggered':
                self.optimize()
                break
            
            time.sleep(trigger_interval)

def main() -> None:
    """Used as entry point to run module from the command line with arguments
    
    The main function is intended for incremental bulk collection and doesn't support all arguments.
    Support for additional arguments can be added as needed.
    """

    args = sys.argv[1:]
    if len(args) < 6:
        print("Incorrect number of arguments provided")
        print("Usage: dbsql_query_logger.py catalog, schema, table, pipeline_mode, backfill_period, reset")
        sys.exit(1)
    
    catalog = sys.argv[1]
    schema = sys.argv[2]
    table = sys.argv[3]
    pipeline_mode = sys.argv[4]
    backfill_period = sys.argv[5]
    reset = sys.argv[6]

    query_logger = QueryLogger(
        catalog = catalog,
        schema = schema,
        table = table,
        pipeline_mode = pipeline_mode,
        backfill_period = backfill_period,
        reset = reset
    )

    query_logger.run()

if __name__ == "__main__":
    logging.basicConfig(
        format="%(asctime)s %(message)s",
        datefmt="%Y-%m-%dT%H:%M:%S%z"
    )
    
    logger = logging.getLogger('dbsql_query_runner')
    logger.setLevel(logging.INFO)
    
    main()